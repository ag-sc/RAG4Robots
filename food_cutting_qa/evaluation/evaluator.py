import csv
import glob
import os
from pathlib import Path

from tqdm import tqdm

from food_cutting_qa.prompting.llama_prompter import LlamaPrompter

quest_path = os.path.join(os.path.dirname(__file__), "..", "qa.csv")
answer_path = os.path.join(os.path.dirname(__file__), "..", "model_answers")
result_path = os.path.join(os.path.dirname(__file__), "..", "results")
evaluator_llm = LlamaPrompter()

sys_msg_scoring = f"Your task is to evaluate answers given by another LLM. You are provided the question, the ground truth answer and the answer given by the other LLM. Given this information, please return a score symbolising the quality of the answer and its closeness to the ground truth. The score should range from 0 to 5, where 5 is the best and 0 is used when the LLM could not provide any answer."
sys_msg_bin = f"Your task is to evaluate answers given by another LLM. You are provided the question, the ground truth answer and the answer given by the other LLM. Given this information, please return True or False depending on whether or not the answer aligns with the ground truth."


def get_model_answer(file: str, question: str) -> str:
    with open(f"{file}", newline='', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            if row["question"] == question:
                return row["model_answer"]
    return ""


# Evaluate the model_answers generated by the different LLMs using an evaluator LLM.
# Differentiate between binary evaluation (Is the answer correct - True/False) and a score-based evaluation (What is the quality of the answer on a scale of 0 (= no answer) to 5 (= best answer)
def evaluate_models(bin_res=True):
    if bin_res:
        sys_msg = sys_msg_bin
        file_n = "binary"
    else:
        sys_msg = sys_msg_scoring
        file_n = "scoring"

    for fname in tqdm(glob.glob(f"{answer_path}/*.csv"), "Evaluating model results"):
        scores = []
        with open(quest_path, "r", encoding="utf-8") as file:
            reader = csv.reader(file)
            header = next(reader)
            for row in reader:
                quest = row[0]
                gt_ans = row[1]
                m_ans = get_model_answer(fname, quest)
                user_msg = f"\nQuestion: {quest}\nGround Truth: {gt_ans}\nLLM Answer: {m_ans}\nResult:"
                score = evaluator_llm.prompt_model(sys_msg, user_msg)
                scores.append((quest, gt_ans, m_ans, score))
        path = Path(fname)
        with open(f"{result_path}/{path.stem}_{file_n}.csv", "w", newline="", encoding="utf-8") as file:
            writer = csv.writer(file)
            writer.writerow(["question", "ground_truth", "model_answer", "score"])
            writer.writerows(scores)


if __name__ == "__main__":
    evaluate_models()
    evaluate_models(False)
